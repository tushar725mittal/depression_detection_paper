{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-02T13:16:54.481220Z","iopub.execute_input":"2023-05-02T13:16:54.482156Z","iopub.status.idle":"2023-05-02T13:17:07.803278Z","shell.execute_reply.started":"2023-05-02T13:16:54.482118Z","shell.execute_reply":"2023-05-02T13:17:07.801617Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Import required libraries\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/depression-dataset/train_data.csv\")\n\n# Split the dataset into training and testing sets\ndf = df.sample(frac=0.01)\ntrain_size = int(0.8 * len(df))\ntrain_data = df[:train_size]\nval_data = df[train_size:]\n\n# Load the BERT tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Encode the training and testing data\ntrain_encodings = tokenizer(train_data[\"text\"].tolist(), truncation=True, padding=True)\nval_encodings = tokenizer(val_data[\"text\"].tolist(), truncation=True, padding=True)\n\n# Create TensorFlow datasets\nwith tf.device('CPU'):\n  train_dataset = (\n      tf.data.Dataset.from_tensor_slices(\n          (dict(train_encodings), train_data[\"label\"].tolist())\n      )\n      .shuffle(10000)\n      .batch(4)\n  )\n\n  test_dataset = tf.data.Dataset.from_tensor_slices(\n      (dict(val_encodings), val_data[\"label\"].tolist())\n  ).batch(4)\n\n# Load the BERT model for sequence classification\nmodel = TFDistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\n# Compile the model\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\nmodel.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n# Train the model\nmodel.fit(train_dataset, epochs=10, validation_data=test_dataset, batch_size=4)\n\n# # Chat with the user to detect depression\n# while True:\n#     text = input(\"How are you feeling today? \")\n#     encoding = tokenizer(text, truncation=True, padding=True, return_tensors='tf')\n#     output = model(encoding)[0]\n#     prediction = tf.argmax(output, axis=1)\n#     if prediction == 1:\n#         print(\"It seems like you might be feeling depressed. Please consider seeking help.\")\n#     else:\n#         print(\"It's great to hear that you're doing well!\")\n\n# Save the model\nmodel.save_pretrained(\"/kaggle/working/depression_bert_model.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T13:17:12.750038Z","iopub.execute_input":"2023-05-02T13:17:12.750466Z","iopub.status.idle":"2023-05-02T13:32:51.554835Z","shell.execute_reply.started":"2023-05-02T13:17:12.750420Z","shell.execute_reply":"2023-05-02T13:32:51.553538Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0205216fd3374945a649ee3f9e16b44e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f4e2ec49744a15971c89ead5fb2a6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6437f9a950f94175b7136a2c34fafde2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d09b8332bd7490a9bce9a470e9e115b"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n458/458 [==============================] - 152s 264ms/step - loss: 1.2057 - accuracy: 0.5614 - val_loss: 0.9065 - val_accuracy: 0.5022\nEpoch 2/10\n458/458 [==============================] - 91s 198ms/step - loss: 0.8973 - accuracy: 0.7051 - val_loss: 1.1248 - val_accuracy: 0.8930\nEpoch 3/10\n458/458 [==============================] - 83s 182ms/step - loss: 0.7428 - accuracy: 0.6832 - val_loss: 1.0517 - val_accuracy: 0.8974\nEpoch 4/10\n458/458 [==============================] - 83s 180ms/step - loss: 0.7147 - accuracy: 0.8443 - val_loss: 1.1651 - val_accuracy: 0.8996\nEpoch 5/10\n458/458 [==============================] - 82s 179ms/step - loss: 0.6763 - accuracy: 0.8542 - val_loss: 2.2989 - val_accuracy: 0.8450\nEpoch 6/10\n458/458 [==============================] - 82s 178ms/step - loss: 0.6691 - accuracy: 0.7979 - val_loss: 1.3461 - val_accuracy: 0.9017\nEpoch 7/10\n458/458 [==============================] - 81s 177ms/step - loss: 0.7202 - accuracy: 0.7597 - val_loss: 1.2556 - val_accuracy: 0.8886\nEpoch 8/10\n458/458 [==============================] - 81s 177ms/step - loss: 0.8208 - accuracy: 0.8225 - val_loss: 1.3561 - val_accuracy: 0.9039\nEpoch 9/10\n458/458 [==============================] - 81s 176ms/step - loss: 0.6400 - accuracy: 0.8280 - val_loss: 1.7867 - val_accuracy: 0.8865\nEpoch 10/10\n458/458 [==============================] - 81s 176ms/step - loss: 0.7197 - accuracy: 0.8591 - val_loss: 1.4592 - val_accuracy: 0.8952\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# saving\nwith open('/kaggle/working/tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-04T18:12:31.616720Z","iopub.execute_input":"2023-05-04T18:12:31.617023Z","iopub.status.idle":"2023-05-04T18:12:31.642998Z","shell.execute_reply.started":"2023-05-04T18:12:31.616995Z","shell.execute_reply":"2023-05-04T18:12:31.641344Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4064573170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working/tokenizer.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n\n# Load the test dataset\ntest_data = pd.read_csv(\"/kaggle/input/depression-dataset/test_data.csv\")\n\n# Encode the test data\ntest_encodings = tokenizer(test_data[\"text\"].tolist(), truncation=True, padding=True)\n\n# Create TensorFlow dataset\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), test_data[\"label\"].tolist())\n).batch(4)\n\n# Evaluate the model on the test dataset\ny_true = []\ny_pred = []\nfor batch in test_dataset:\n    batch_input = batch[0]\n    batch_labels = batch[1]\n    batch_output = model(batch_input)[0].numpy()\n    batch_pred = np.argmax(batch_output, axis=1)\n    y_true.extend(batch_labels)\n    y_pred.extend(batch_pred)\n\n# Compute evaluation metrics\nacc = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred)\nauc_roc = roc_auc_score(y_true, y_pred)\n\nprint(\"Accuracy: {:.4f}\".format(acc))\nprint(\"Precision: {:.4f}\".format(precision))\nprint(\"Recall: {:.4f}\".format(recall))\nprint(\"F1 Score: {:.4f}\".format(f1))\nprint(\"AUC-ROC: {:.4f}\".format(auc_roc))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T13:46:52.639280Z","iopub.execute_input":"2023-05-02T13:46:52.640095Z","iopub.status.idle":"2023-05-02T14:01:07.119352Z","shell.execute_reply.started":"2023-05-02T13:46:52.640045Z","shell.execute_reply":"2023-05-02T14:01:07.117952Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Accuracy: 0.9127\nPrecision: 0.8731\nRecall: 0.9660\nF1 Score: 0.9172\nAUC-ROC: 0.9127\n","output_type":"stream"}]},{"cell_type":"code","source":"# print confusion matrix all 4 parts\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))\nprint(\"True Negatives: {}\".format(confusion_matrix(y_true, y_pred)[0][0]))\nprint(\"False Negatives: {}\".format(confusion_matrix(y_true, y_pred)[1][0]))\nprint(\"True Positives: {}\".format(confusion_matrix(y_true, y_pred)[1][1]))\nprint(\"False Positives: {}\".format(confusion_matrix(y_true, y_pred)[0][1]))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T14:01:10.406186Z","iopub.execute_input":"2023-05-02T14:01:10.406653Z","iopub.status.idle":"2023-05-02T14:01:14.133824Z","shell.execute_reply.started":"2023-05-02T14:01:10.406603Z","shell.execute_reply":"2023-05-02T14:01:14.132540Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[11162  1827]\n [  443 12573]]\nTrue Negatives: 11162\nFalse Negatives: 443\nTrue Positives: 12573\nFalse Positives: 1827\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport tensorflow as tf\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n\n# Load the saved tokenizer\ntokenizer_path = '/kaggle/working/tokenizer.pickle'\nwith open(tokenizer_path, 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\n# Load the saved model\nmodel_path = '/kaggle/working/depression_bert_model.h5'\nmodel = TFDistilBertForSequenceClassification.from_pretrained(model_path)\n\n# Define the sentence to predict\nsentence = \"I'm feeling sad today\"\n\n# Tokenize and preprocess the input sentence\ninputs = tokenizer(sentence, truncation=True, padding=True, return_tensors='tf')\n\n# Make the prediction\noutput = model(inputs)[0]\nprediction = tf.argmax(output, axis=1)\n\n# Print the predicted value\nif prediction == 1:\n    print(\"It seems like you might be feeling depressed. Please consider seeking help.\")\nelse:\n    print(\"It's great to hear that you're doing well!\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T08:34:27.107612Z","iopub.execute_input":"2023-05-25T08:34:27.108361Z","iopub.status.idle":"2023-05-25T08:34:38.567300Z","shell.execute_reply.started":"2023-05-25T08:34:27.108314Z","shell.execute_reply":"2023-05-25T08:34:38.565844Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2546602392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/working/tokenizer.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Ran out of input"],"ename":"EOFError","evalue":"Ran out of input","output_type":"error"}]}]}